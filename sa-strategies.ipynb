{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4520835-0e3c-430d-a665-fe6e60c44f81",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Comments and suggestions: `fernando.batista@iscte-iul.pt`\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fmmb/ADD_In-2023/blob/main/sa-strategies.ipynb)\n",
    "\n",
    "Table of contents\n",
    "* [Setup](#intro)\n",
    "* [Approach 1: Using existing NLP tools](#approach1)\n",
    "* [Approach 2: Using a sentiment lexicon](#approach2)\n",
    "* [Approach 3: Using pre-trained transformer-based models](#approach3)\n",
    "* [Approach 4: Machine Learning - Training your own classifier from scratch](#approach4)\n",
    "* [Approach 5: ChatGPT-like models](#approach5)\n",
    "\n",
    "If you are using google colab, please check the [instructions on how to use your files in google colab](README.md#files-in-google-colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73c523-fef6-441b-804f-59fb1fea77a9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9e49c-4d10-434a-ab4e-651aec750a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fc1fd-85bc-4fc0-8520-82e2720bdf05",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p id=\"approach1\"></p>\n",
    "\n",
    "## Approach 1: Using existing NLP tools\n",
    "There are many existing tools for sentiment analysis, such as `TextBlob` and `sentiment Vader`, amongst others. This section presents examples on how to use these two tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdc93f-1728-4893-911d-2650151381af",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "`TextBlob` will be used to perform our initial sentiment analysis tests.\n",
    "Please consider the following list of texts, stored in the variable `texts` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a46c54-7a1b-4fae-8f16-69637a272080",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [ \"The movie was good.\",\n",
    "          \"I hate the movie\",\n",
    "          \"The movie was not good.\",\n",
    "          \"I really think this product sucks.\",\n",
    "          \"Really great product.\",\n",
    "          \"I don't like this product\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a36f1-3653-442b-9b76-0929f685679c",
   "metadata": {},
   "source": [
    "We can now print each one of the individual texts, together with it's sentiment score. A value above 0 means that the sentiment is positive, while a value below 0, means it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f998e0-7aa1-4127-b637-b42b0dacb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in texts:\n",
    "    print(t, \"==>\", TextBlob(t).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21131a4f-080c-43aa-b42b-7ffc4df98af6",
   "metadata": {},
   "source": [
    "The previous code assumes that the text is already split into sentences, which may not be the case of texts comming from sources, such as *web pages* or *blogs*. However, your can give the whole text to `textblob`, and it is able to automatically split the texts into sentences, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d79d-77d4-4657-b281-b8f0a9fefbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"\"\"The movie was good. The movie was not good. I really think this product sucks.\n",
    "Really great product. I don't like this product.\"\"\"\n",
    "text=TextBlob(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8db79-a424-407b-8d3b-3b1a52473da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in text.sentences:\n",
    "    print(s, \"==> \", s.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a1454-57f4-4c01-84e4-cd00fd75c36d",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's automatically classify a set of texts and evaluate the corresponding performance. \n",
    "\n",
    "In order to be able to do that, one has to provide the true labels together with the data. However, please note that the classification system won't be able to see the true labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c2ebc-eaf8-4b25-8eac-aba3d5b59533",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I love chocolate\",\n",
    "        \"I hate to eat\", \n",
    "        \"I don't love anyone\",\n",
    "        \"I like cakes\",\n",
    "        \"I don't fail\"]\n",
    "tags=[\"pos\", \"neg\", \"neg\", \"pos\", \"pos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea590b-1963-4891-a84d-c37e40ac0994",
   "metadata": {},
   "source": [
    "Let's do a small test with one of these examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fe65d-aab4-49e2-8ec9-09c3ea61dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEXT:\", texts[0])\n",
    "print(\"SENTIMENT:\", TextBlob(texts[0]).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa09316-1f7a-4e98-bf5b-6390f7fb4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "incorrect=0\n",
    "for i in range(len(texts)):\n",
    "    polarity = TextBlob(texts[i]).sentiment.polarity\n",
    "    print(f\"SCORE: {polarity:4}, TAG: {tags[i]}, TEXT: {texts[i]}\")\n",
    "    if polarity >=0 and tags[i] == \"pos\":\n",
    "        correct +=1\n",
    "    elif polarity <0 and tags[i] == \"neg\":\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "accuracy=(correct)/(correct+incorrect)\n",
    "print(f\"correct: {correct}, incorrect: {incorrect}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fbce4-3d50-4c6d-a72a-a738a2c7571f",
   "metadata": {},
   "source": [
    "### Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7b80c-2bf7-4282-b5f1-567eba2eb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50622de3-9bbd-46fb-8556-634c2e7b231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "texts = [\n",
    "    \"This food is amazing and tasty !\",\n",
    "    \"Exoplanets are planets outside the solar system\",\n",
    "    \"It is sad to see such a bad behavior\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    score = sa.polarity_scores(text)[\"compound\"]\n",
    "    print('TEXT:', text)\n",
    "    print('  SENTIMENT:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6bd1a-d351-4ec5-92c8-44fef0eb2d2f",
   "metadata": {},
   "source": [
    "After computing the polarity score, we can also calculate the proportion of each sentiment in a sentence using the keys: \"pos\", \"neu\" and \"neg\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c762561-c7d6-4319-930c-25b0afe89f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    scores = sa.polarity_scores(text)\n",
    "    print('TEXT:', text)\n",
    "    print('  SCORES:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe16b17-c84c-4abf-a76a-4d7111ec3c2a",
   "metadata": {},
   "source": [
    "### Flair (optional)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ecfd220-39cc-4657-9453-ba598d13d17e",
   "metadata": {},
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f473c695-fc86-42ee-9fb7-ab105e830d99",
   "metadata": {},
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "sentence = Sentence('I love Lisbon .')\n",
    "\n",
    "# load and apply the Sentiment tagger\n",
    "tagger = Classifier.load('sentiment')\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with all annotations\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02e559-f180-4802-abcc-10b7d3d11bf6",
   "metadata": {},
   "source": [
    "<p id=\"approach2\"></p>\n",
    "\n",
    "## Approach 2: Applying a sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9499e9-f9a4-4cc0-b0f7-1fbcf3277612",
   "metadata": {},
   "source": [
    "Let's start by reading the sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19dfc9-3834-443a-a610-7ad39bc73f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexname=\"https://raw.githubusercontent.com/fmmb/ADD_In-2023/main/sample-lexicon.csv\"\n",
    "data = pd.read_csv(lexname, encoding=\"utf-8\", index_col=[\"English\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dff45-5486-4165-a1b2-e6eb71dce0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b02aa-7e69-475b-af06-c402eb2ea70b",
   "metadata": {},
   "source": [
    "We will be using the dictionary `lex` instead of the dataframe, since the performance is much better that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bce2a-1534-4a9d-bb25-85c8e6a2be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = data['polarity'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658a9b4-9292-4f76-a45f-56de555a0888",
   "metadata": {},
   "source": [
    "Let's test some well-known words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8044132-dd94-4593-aa01-4ebc8251c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['good', 'ok', 'dislike', 'bad']:\n",
    "    print(f\"WORD: {w}, POLARITY: {lex.get(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819c436-7f41-4872-99bb-aa985cb3d6bd",
   "metadata": {},
   "source": [
    "Now, to calculate the semntiment of a sentence, we only have to sum the polarity of each individual word, and check the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e85c08-1199-4748-b1c4-710d3bb531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a lovely and beautiful place , but I hate the corner'\n",
    "polarity = 0\n",
    "for w in text.split():\n",
    "    polarity += lex.get(w, 0)\n",
    "    print(w, lex.get(w, 0) )\n",
    "print(\"Final score:\" , polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc1624-9b2a-4d19-abee-acff007d4a41",
   "metadata": {},
   "source": [
    "Even better with a function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5799f9-6b82-4d11-9dfa-c76b9375caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    score = 0\n",
    "    for w in text.split():\n",
    "        score += lex.get(w, 0)\n",
    "    if score >= 0:\n",
    "        return \"POS\"\n",
    "    else:\n",
    "        return \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfef94-9366-49e4-a7fc-6a7d8fb51cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a lovely and beautiful place , but I hate the corner'\n",
    "sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bf77f-aa88-4c1a-992d-456b92eacd9e",
   "metadata": {},
   "source": [
    "We could now process all our texts again with our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef58b63-e8b3-4e2c-9d6f-232d06b1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    print(text, \"=>\", sentiment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347968a0-11a4-4f08-8245-a8be61a06b5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p id=\"approach3\"></p>\n",
    "\n",
    "## Approach 3: Using pre-trained transformer-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089b949-473c-4fa4-b427-fedfeaa372e9",
   "metadata": {},
   "source": [
    "### The easy way ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0db832-1c98-485f-aae1-520a5026ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549af846-d1a5-4165-bd24-abb5ec28f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8d357-6b86-4317-b8ba-43e95006c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "#sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013bb23-b285-498e-a75d-8f05d59a7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love you\", \"I hate you\", \"the unemployment is increasing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71115f01-d260-4bac-8eac-91452ecde5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc01e1-9ae3-44f6-8da7-3bff4c802517",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tunning (optional)\n",
    "\n",
    "Some practical readings...\n",
    "* [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python)\n",
    "* [Sentiment Analysis in 10 Minutes with BERT and TensorFlow](https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7026da9-4096-48fb-8fc7-0ad721755170",
   "metadata": {},
   "source": [
    "<p id=\"approach3\"></p>\n",
    "\n",
    "## Approach 4: Training your own classifier from scratch (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41947640-c329-40bc-a7be-1dfb4338b926",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets use [Sentiment Polarity Dataset 2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), included in the `NLTK` library. It consists of 1000 positive and 1000 negative processed reviews. Introduced in Pang/Lee ACL 2004. Released June 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe89e0-7f4a-4722-8f1e-9e8e154ebd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['movie_reviews','punkt','stopwords','averaged_perceptron_tagger'])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71815e37-102c-4f5a-b248-0c9402bcc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "print(\"The data contains %d reviews\"% len(mr.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41155b81-6bb7-466b-8b2e-528b9a59136c",
   "metadata": {},
   "source": [
    "### Shuffling the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93103b73-1df5-4f61-a9ce-0ac720fdac17",
   "metadata": {},
   "source": [
    "We start by shuffling the documents, otherwise they will remain sorted [\"neg\", \"neg\" ... \"pos\"]. Then we will proceed using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180224ac-8664-4d7d-b519-345bbeb0ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "docnames=mr.fileids()\n",
    "random.shuffle(docnames)\n",
    "\n",
    "# create two separate lists: documents and tags\n",
    "documents=[]\n",
    "tags = []\n",
    "for doc in docnames:\n",
    "    documents.append(mr.raw(doc))\n",
    "    tags.append( doc.split('/')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a452a-9218-433e-9d39-33df0c6cd48a",
   "metadata": {},
   "source": [
    "Let's check the first few documents ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb83ae-15d9-46ef-b9f4-10ee0e7e9a4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"DOC:\", documents[i][:400])\n",
    "    print(\"TAG:\", tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86403678-6aff-4476-ae6e-e3ac494d4673",
   "metadata": {},
   "source": [
    "The first 80% of the documents will be used for training, and the final 20% will be used for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b67ffd-07df-4711-8f7f-2a5f0931507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = int(len(documents) * 80 / 100)  # number of training documents\n",
    "train_documents, test_documents = documents[:numtrain], documents[numtrain:]\n",
    "train_tags, test_tags = tags[:numtrain], tags[numtrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df12f4-595a-4566-9f7d-cf072717330f",
   "metadata": {},
   "source": [
    "Now that we have separated training and testing sets, we will convert the texts into their vectorial representation. Scikit-learn provides two interesting methods for this: `CountVectorizer` and `TfidfVectorizer`. Please check the documentation if you want to check different parameters.\n",
    "- [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5d7cc-fc44-4110-821a-42af80a84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_documents)\n",
    "test_X = vectorizer.transform(test_documents)\n",
    "print(\"TRAIN SIZE:\", train_X.shape)\n",
    "print(\"TEST SIZE:\", test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c6062-2e6d-47f9-b0bc-2d1c421093e3",
   "metadata": {},
   "source": [
    "We can see that the features are actually the words from the texts, where some strange \"words\" can also be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ca76f-4802-4104-8b66-8eb7bfe24a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[600:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006179-ab4a-47cf-81e1-560fc9149e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c14cc-67c4-484e-9a33-da08f974ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_X, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be3c92-c2f7-4469-b6c7-bd1c434c3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a7bc-294c-4feb-9975-bd911d748907",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sklearn.metrics.accuracy_score(test_tags, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea54cf7-89d3-4c8e-ad07-b9d52b79baa2",
   "metadata": {},
   "source": [
    "### Using the classifier for processing new texts\n",
    "\n",
    "1. Please note that you have to perform the exact same processing steps to the new sentences, previously used during training.\n",
    "2. Then, you have only to apply the classifier to the new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e760d13-e44f-4dbb-85cd-3ee533678cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"I love movies very much\", \n",
    "          \"I hate my stupid life\",\n",
    "          \"I am disapointed with the argument\"]\n",
    "frases_X = vectorizer.transform(frases)\n",
    "classifier.predict(frases_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33198dbb-b565-418e-bb42-8431d420a84f",
   "metadata": {},
   "source": [
    "<p id=\"approach4\"></p>\n",
    "\n",
    "## Approach 5: ChatGPT-like models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e66f6-c31e-4908-99e6-fc05f4dea11c",
   "metadata": {},
   "source": [
    "Please use [ChatGPT](https://chat.openai.com/) or a similar model, such as [BARD](https://bard.google.com/), to calculate the sentiment of the following sentences\n",
    "* This food is amazing and tasty!\n",
    "* Exoplanets are planets outside the solar system\n",
    "* It is sad to see such a bad behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45e644-0411-4912-bb5a-375ac5590919",
   "metadata": {},
   "source": [
    "Try changing the prompt as follows:\n",
    "> Calculate the sentiment of the following sentences. Produce the output in the following format: TEXT, SENTIMENT (Positive, Neutral, Negative)\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968cf97-540e-45ba-a1df-a9002b100f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
