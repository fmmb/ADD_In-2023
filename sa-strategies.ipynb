{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4520835-0e3c-430d-a665-fe6e60c44f81",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Comments and suggestions: `fernando.batista@iscte-iul.pt`\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fmmb/ADD_in/blob/main/SA.ipynb)\n",
    "\n",
    "Table of contents\n",
    "* [Intro](#intro)\n",
    "* [Approach 1: Using existing NLP tools](#approach1)\n",
    "* [Approach 2: Using a sentiment lexicon](#approach2)\n",
    "* [Approach 3: Machine Learning - Training your own classifier](#approach3)\n",
    "* [Approach 4: Using pre-trained transformer-based models](#approach4)\n",
    "* [Approach 5: ChatGPT-like models](#approach5)\n",
    "\n",
    "If you are using google colab, please check the [instructions on how to use your files in google colab](README.md#files-in-google-colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73c523-fef6-441b-804f-59fb1fea77a9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9e49c-4d10-434a-ab4e-651aec750a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fc1fd-85bc-4fc0-8520-82e2720bdf05",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p id=\"approach1\"></p>\n",
    "\n",
    "## Approach 1: Using existing NLP tools\n",
    "There are many existing tools for sentiment analysis, such as `TextBlob` and `sentiment Vader`, amongst others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdc93f-1728-4893-911d-2650151381af",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "`TextBlob` is one of these NLP tools and will be used to perform our initial tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a46c54-7a1b-4fae-8f16-69637a272080",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [ \"The movie was good.\",\n",
    "          \"I hate the movie\",\n",
    "          \"The movie was not good.\",\n",
    "          \"I really think this product sucks.\",\n",
    "          \"Really great product.\",\n",
    "          \"I don't like this product\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f998e0-7aa1-4127-b637-b42b0dacb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in texts:\n",
    "    print(t, \"==>\", TextBlob(t).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21131a4f-080c-43aa-b42b-7ffc4df98af6",
   "metadata": {},
   "source": [
    "The previous code assumes that the text is already split into sentences, which may not be the case of texts comming from sources, such as *web pages* or *blogs*. An alternate solution would be to give the whole text to `textblob` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d79d-77d4-4657-b281-b8f0a9fefbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"\"\"The movie was good. The movie was not good. I really think this product sucks.\n",
    "Really great product. I don't like this product.\"\"\"\n",
    "text=TextBlob(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8db79-a424-407b-8d3b-3b1a52473da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in text.sentences:\n",
    "    print(s, \"==> \", s.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a1454-57f4-4c01-84e4-cd00fd75c36d",
   "metadata": {},
   "source": [
    "### Evaluation: classifying a set of texts and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c2ebc-eaf8-4b25-8eac-aba3d5b59533",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I love chocolate\",\n",
    "        \"I hate to eat\", \n",
    "        \"I don't love anyone\",\n",
    "        \"I like cakes\",\n",
    "        \"I don't fail\"]\n",
    "tags=[\"pos\", \"neg\", \"neg\", \"pos\", \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fe65d-aab4-49e2-8ec9-09c3ea61dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(texts[0]).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa09316-1f7a-4e98-bf5b-6390f7fb4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "incorrect=0\n",
    "for i in range(len(texts)):\n",
    "    polarity = TextBlob(texts[i]).sentiment.polarity\n",
    "    print(f\"  => {polarity:4}, {tags[i]}, {texts[i]}\")\n",
    "    if polarity >=0 and tags[i] == \"pos\":\n",
    "        correct +=1\n",
    "    elif polarity <0 and tags[i] == \"neg\":\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "acc=(correct)/(correct+incorrect)\n",
    "print(f\"correct: {correct}, incorrect: {incorrect}, accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fbce4-3d50-4c6d-a72a-a738a2c7571f",
   "metadata": {},
   "source": [
    "### Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7b80c-2bf7-4282-b5f1-567eba2eb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50622de3-9bbd-46fb-8556-634c2e7b231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentences = [\n",
    "    \"This food is amazing and tasty !\",\n",
    "    \"Exoplanets are planets outside the solar system\",\n",
    "    \"This is sad to see such bad behavior\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    score = sa.polarity_scores(sentence)[\"compound\"]\n",
    "    print(f'{sentence}. sentiment score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6bd1a-d351-4ec5-92c8-44fef0eb2d2f",
   "metadata": {},
   "source": [
    "We can also calculate the percentage of each sentiment present in that sentence using \"pos\", \"neu\" and \"neg\" keys after computing the polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c762561-c7d6-4319-930c-25b0afe89f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    polarity = sa.polarity_scores(sentence)\n",
    "    print(f\"{sentence}\\n  => {polarity}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe16b17-c84c-4abf-a76a-4d7111ec3c2a",
   "metadata": {},
   "source": [
    "### Flair (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e35631-9d03-4020-9a07-7e2d3ebc4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fd004c-6caa-4da8-912a-54ddb984d156",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer_utils because of the following error (look up to see its traceback):\n'EntryPoint' object has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:1130\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:848\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer_utils.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseed_worker\u001b[39m(_):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py:51\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v2/__init__.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v2/compat/__init__.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py:327\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m    328\u001b[0m   _current_module\u001b[38;5;241m.\u001b[39m__path__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    329\u001b[0m       [_module_util\u001b[38;5;241m.\u001b[39mget_parent_dir(summary)] \u001b[38;5;241m+\u001b[39m _current_module\u001b[38;5;241m.\u001b[39m__path__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/summary/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/summary/v1.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_scalar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _custom_scalar_summary\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _histogram_summary\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _image_summary\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/plugins/histogram/summary.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary_v2\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Export V3 versions.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/plugins/histogram/summary_v2.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_tensor_creator\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[1;32m     38\u001b[0m DEFAULT_BUCKET_COUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/util/tensor_util.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow_stub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes, compat, tensor_shape\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mExtractBitsFromFloat16\u001b[39m(x):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/compat/tensorflow_stub/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_dtype  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DType  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/compat/tensorflow_stub/pywrap_tensorflow.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[1;32m     25\u001b[0m TFE_DEVICE_PLACEMENT_WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/compat/tensorflow_stub/io/__init__.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     FSSPEC_ENABLED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fsspec/__init__.py:45\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load filesystem from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 45\u001b[0m register_implementation(spec\u001b[38;5;241m.\u001b[39mname, \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m, errtxt\u001b[38;5;241m=\u001b[39merr_msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EntryPoint' object has no attribute 'module'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sentence\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Classifier\n\u001b[1;32m      4\u001b[0m sentence \u001b[38;5;241m=\u001b[39m Sentence(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI love Lisbon .\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/flair/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed \u001b[38;5;28;01mas\u001b[39;00m hf_set_seed\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# global variable: cache_root\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_proxies\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:1120\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1120\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:1132\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1135\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer_utils because of the following error (look up to see its traceback):\n'EntryPoint' object has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "sentence = Sentence('I love Lisbon .')\n",
    "\n",
    "# load and apply the Sentiment tagger\n",
    "tagger = Classifier.load('sentiment')\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with all annotations\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7530fd-0e0b-4059-bb1e-a1d9a5c18879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c02e559-f180-4802-abcc-10b7d3d11bf6",
   "metadata": {},
   "source": [
    "<p id=\"approach2\"></p>\n",
    "\n",
    "## Approach 2: Applying a sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9499e9-f9a4-4cc0-b0f7-1fbcf3277612",
   "metadata": {},
   "source": [
    "Leitura do léxico de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19dfc9-3834-443a-a610-7ad39bc73f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"../data/en/NCR-lexicon.csv\", encoding=\"utf-8\")\n",
    "data = pd.read_csv(\"sample-lexicon.csv\", encoding=\"utf-8\", index_col=[\"English\"])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bce2a-1534-4a9d-bb25-85c8e6a2be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = data['polarity'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e85c08-1199-4748-b1c4-710d3bb531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I hate to say goodbye but I love chocolate'\n",
    "polarity = 0\n",
    "for w in text.split():\n",
    "    polarity += lex.get(w, 0)\n",
    "    print(w, lex.get(w, 0) )\n",
    "print(\"Sum:\" , polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc1624-9b2a-4d19-abee-acff007d4a41",
   "metadata": {},
   "source": [
    "Even better with a function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5799f9-6b82-4d11-9dfa-c76b9375caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    polarity = 0\n",
    "    for w in text.split():\n",
    "        polarity += lex.get(w, 0)\n",
    "    if polarity >= 0:\n",
    "        return \"POS\"\n",
    "    else:\n",
    "        return \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfef94-9366-49e4-a7fc-6a7d8fb51cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I hate to say goodbye but I love chocolate'\n",
    "sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef58b63-e8b3-4e2c-9d6f-232d06b1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    print(sentiment(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7026da9-4096-48fb-8fc7-0ad721755170",
   "metadata": {},
   "source": [
    "<p id=\"approach3\"></p>\n",
    "\n",
    "## Approach 3: Training your own classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41947640-c329-40bc-a7be-1dfb4338b926",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets use [Sentiment Polarity Dataset 2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), included in the `NLTK` library.<Br>\n",
    "It consists of 1000 positive and 1000 negative processed reviews. Introduced in Pang/Lee ACL 2004. Released June 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe89e0-7f4a-4722-8f1e-9e8e154ebd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['movie_reviews','punkt','stopwords','averaged_perceptron_tagger'])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71815e37-102c-4f5a-b248-0c9402bcc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "print(\"The corpus contains %d reviews\"% len(mr.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41155b81-6bb7-466b-8b2e-528b9a59136c",
   "metadata": {},
   "source": [
    "### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93103b73-1df5-4f61-a9ce-0ac720fdac17",
   "metadata": {},
   "source": [
    "Lets shuffle the documents, otherwise they will remain sorted [\"neg\", \"neg\" ... \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd18ef-f884-4aeb-a7b6-b57975cb1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "docnames=mr.fileids()\n",
    "random.shuffle(docnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34146da-5dbc-4a07-96fe-8be58c1486ae",
   "metadata": {},
   "source": [
    "### Let's do it using some useful scikit-learn functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c10faf-e6a4-4519-841c-e92817394bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727f71c-2b4e-4a48-99d0-20f5d613cbf3",
   "metadata": {},
   "source": [
    "#### Assuming that documents are shuffled\n",
    "Make sure `docnames` contain a shuffled list of documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180224ac-8664-4d7d-b519-345bbeb0ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "tags = []\n",
    "for doc in docnames:\n",
    "    documents.append(mr.raw(doc))\n",
    "    tags.append( doc.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb83ae-15d9-46ef-b9f4-10ee0e7e9a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"{} -> {}...\".format(tags[i], documents[i][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b67ffd-07df-4711-8f7f-2a5f0931507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = int(len(documents) * 80 / 100)  # number of training documents\n",
    "train_documents, test_documents = documents[:numtrain], documents[numtrain:]\n",
    "train_tags, test_tags = tags[:numtrain], tags[numtrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df12f4-595a-4566-9f7d-cf072717330f",
   "metadata": {},
   "source": [
    "Agora que temos os conjuntos de treino e de teste separados, há que converter o texto dos documentos na sua representação vetorial. O scikit-learn tem dois métodos interessantes: `CountVectorizer` e `TfidfVectorizer`. Ambos aceitam um conjunto interessante de parâmetros, que não exploramos aqui, mas que vale a pena consultar.\n",
    "- [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5d7cc-fc44-4110-821a-42af80a84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_documents)\n",
    "test_X = vectorizer.transform(test_documents)\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c6062-2e6d-47f9-b0bc-2d1c421093e3",
   "metadata": {},
   "source": [
    "Podemos verificar que as features são na verdade as \"palavras\" dos textos, onde também se incluem números e outras coisas extranhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ca76f-4802-4104-8b66-8eb7bfe24a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[600:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006179-ab4a-47cf-81e1-560fc9149e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c14cc-67c4-484e-9a33-da08f974ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_X, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be3c92-c2f7-4469-b6c7-bd1c434c3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a7bc-294c-4feb-9975-bd911d748907",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sklearn.metrics.accuracy_score(test_tags, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea54cf7-89d3-4c8e-ad07-b9d52b79baa2",
   "metadata": {},
   "source": [
    "### Using the classifier for processing new texts\n",
    "\n",
    "1. Please note that you have to perform the exact same processing steps to the new sentences, previously used during training.\n",
    "2. Then, you have only to apply the classifier to the new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e760d13-e44f-4dbb-85cd-3ee533678cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"I love movies very much\", \n",
    "          \"I hate my stupid life\",\n",
    "          \"I am disapointed with the argument\"]\n",
    "frases_X = vectorizer.transform(frases)\n",
    "classifier.predict(frases_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347968a0-11a4-4f08-8245-a8be61a06b5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p id=\"approach4\"></p>\n",
    "\n",
    "## Approach 4: Using pre-trained transformer-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089b949-473c-4fa4-b427-fedfeaa372e9",
   "metadata": {},
   "source": [
    "### The easy way ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0db832-1c98-485f-aae1-520a5026ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549af846-d1a5-4165-bd24-abb5ec28f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8d357-6b86-4317-b8ba-43e95006c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "#sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013bb23-b285-498e-a75d-8f05d59a7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love you\", \"I hate you\", \"the unemployment is increasing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71115f01-d260-4bac-8eac-91452ecde5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc01e1-9ae3-44f6-8da7-3bff4c802517",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tunning (optional)\n",
    "\n",
    "Some practical readings...\n",
    "* [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python)\n",
    "* [Sentiment Analysis in 10 Minutes with BERT and TensorFlow](https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33198dbb-b565-418e-bb42-8431d420a84f",
   "metadata": {},
   "source": [
    "<p id=\"approach4\"></p>\n",
    "\n",
    "## Approach 5: ChatGPT-like models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e66f6-c31e-4908-99e6-fc05f4dea11c",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03f5f1-2461-4651-b77c-95d9758479ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
